# Citiâ€¯Bike Demandâ€‘Forecasting â€” Migration Plan

_(adapted from the NYCâ€‘Taxi MLOps project)_
**Last updatedÂ Â·Â 2025â€‘05â€‘09**

---

## Project Goals

- Reâ€‘use the taxi codebase to forecast **hourly Citiâ€¯Bike demand per station**.
- Keep APIs/fileâ€‘formats unchanged wherever possible.
- Store features, models, and predictions in **Hopsworks**; surface live metrics with **Streamlit CommunityÂ Cloud**.

---

## Project Folder Structure

The repository root is **`CITIBIKE_RIDES/`** (screenshot from VSÂ Code shown earlier).
Below is the canonical layout after migrationÂ â€” notebooks, pipelines and code folders match the taxi repo, while raw data mirrors the Citiâ€¯Bike download buckets.

```
CITIBIKE_RIDES
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â”œâ”€â”€ 2023-citibike-tripdata
â”‚   â”‚   â”‚   â”œâ”€â”€ 202301â€‘citibikeâ€‘tripdata
â”‚   â”‚   â”‚   â”œâ”€â”€ â€¦ (one folder per month) â€¦
â”‚   â”‚   â”‚   â””â”€â”€ 202312â€‘citibikeâ€‘tripdata
â”‚   â”‚   â””â”€â”€ zip_files/                     # archived zips (kept for provenance)
â”‚   â””â”€â”€ processed
â”‚       â””â”€â”€ 2023/citibike_2023_all.parquet # single consolidated parquet (created)
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ frontend_monitor.py                # MAE / MAPE dashboard
â”‚   â””â”€â”€ frontend_v2.py                     # circleâ€‘map analytics dashboard
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ 01_fetch_data.ipynb
â”‚   â”œâ”€â”€ 02_validate_and_save.ipynb
â”‚   â”œâ”€â”€ 03_transform_processed_data_into_ts_data.ipynb
â”‚   â”œâ”€â”€ 04_transform_ts_data_into_features_and_targets.ipynb
â”‚   â”œâ”€â”€ 05_transform_raw_data_into_features_and_targets.ipynb
â”‚   â”œâ”€â”€ 06_visualization.ipynb
â”‚   â”œâ”€â”€ 07_baseline_models.ipynb
â”‚   â”œâ”€â”€ 08_xgboost_model.ipynb
â”‚   â”œâ”€â”€ 09_lightgbm_model.ipynb
â”‚   â”œâ”€â”€ 10_lgm_with_fe.ipynb
â”‚   â”œâ”€â”€ 11_lgm_hyper.ipynb
â”‚   â”œâ”€â”€ 12_load_features_hopsworks.ipynb
â”‚   â”œâ”€â”€ 13_feature_pipeline.ipynb
â”‚   â”œâ”€â”€ 14_model_training_pipeline.ipynb
â”‚   â”œâ”€â”€ 15_predict_using_hopsworks_model.ipynb
â”‚   â”œâ”€â”€ 16_inference_pipeline.ipynb
â”‚   â”œâ”€â”€ 17_fetch_predictions.ipynb
â”‚   â”œâ”€â”€ 18_plot_mae.ipynb
â”‚   â”œâ”€â”€ 19_retraining_model.ipynb
â”‚   â”œâ”€â”€ 20_hyperparameter_tuning.ipynb
â”‚   â”œâ”€â”€ 21_fft_arma_arima_prophet.ipynb
â”‚   â”œâ”€â”€ nyc_taxi_predictions.html           # legacy artefact (kept for reference)
â”‚   â”œâ”€â”€ test_frontend_v2.ipynb
â”‚   â””â”€â”€ Untitled3.ipynb
â”œâ”€â”€ pipelines
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inference_pipeline.py
â”‚   â””â”€â”€ model_training_pipeline.py
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ experiment_utils.py
â”‚   â”œâ”€â”€ feature_pipeline.py
â”‚   â”œâ”€â”€ frontend_v1.py
â”‚   â”œâ”€â”€ frontend.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ pipeline_utils.py
â”‚   â””â”€â”€ plot_utils.py
â”œâ”€â”€ test
â”‚   â””â”€â”€ sample_app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements_feature_pipeline.txt
â”œâ”€â”€ requirements_with_version.txt
â”œâ”€â”€ requirements_final.txt
â”œâ”€â”€ todo.md
â”œâ”€â”€ vscode_config.json
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

> **Note**: the migration scripts will **create** the `processed/` tree and Parquet files; everything else already exists in the repo.

---

## Rawâ€‘Data Consolidation

- **Input**Â : 53Â CSV files (\~6.9â€¯GB) in `data/raw/2023â€‘*/`.
- **Output**Â : `data/processed/2023/citibike_2023_all.parquet` (PyArrow).
- **Steps**

  1. Chunkâ€‘read (1â€¯M rows) â†’ cast to canonical schema.
  2. Drop _exactâ€‘row_ duplicate `ride_id`s; log the count.
  3. External mergeâ€‘sort by `started_at`, `ride_id`.
  4. Write Parquet with `pyarrow` compression (`snappy`).

- **Canonical columns**:
  `ride_id, rideable_type, started_at, ended_at, start_station_name, start_station_id, end_station_name, end_station_id, start_lat, start_lng, end_lat, end_lng, member_casual`
- **Timezone**: treat datetimes as naÃ¯ve **America/New_York**.

---

## Feature Engineering

1. **Aggregate** rides per `start_station_id` Ã— `start_hour` â†’ `ride_count`.
2. **Lag features**: `rides_tâ€‘1 â€¦ rides_tâ€‘672` (28â€¯days).
3. **Calendar features**: hour & weekday sine/cosine, holiday flag.
4. **Rolling 30â€‘day popularity**.
5. **Rider mix**: `member_share`, `share_electric`, `share_classic`.
6. **Label** column added as `"target"` (copy of `ride_count`).
7. **Station coordinates**: first run extracts unique `(start_station_id, lat, lon)` into memory; optional `stations_fg` for reuse.

---

## Feature Store (Hopsworks)

| Object        | Name                      | Version | Primary Key                         | Event Time        |
| ------------- | ------------------------- | ------- | ----------------------------------- | ----------------- |
| FeatureÂ Group | `bike_hourly_fg`          | 1       | `start_station_id`                  | `start_hour`      |
| FeatureÂ View  | `bike_hourly_fv`          | 1       | â€”                                   | â€”                 |
| PredictionÂ FG | `bike_demand_predictions` | 1       | `start_station_id, prediction_hour` | `prediction_hour` |

---

## Training & Validation

- **Dataset window**: 2023â€‘01â€‘01Â â†’Â 2023â€‘12â€‘31.
- **Split**:

  - Train â‰¤â€¯2023â€‘08â€‘31
  - Validation 2023â€‘09â€‘01Â â†’Â 2023â€‘10â€‘31
  - Test â‰¥â€¯2023â€‘11â€‘01

- **Model**: LightGBM (regression_l1).
- **Hyperâ€‘parameter tuning**: Optuna, 75 trials, best pipeline saved as `citibike_lgbm_optuna_best`.

---

## Drift & Retraining

- Compute 7â€‘day rolling MAE from `bike_demand_predictions`.
- **Trigger** retrain when weekly MAEâ€¯>â€¯**1.5â€¯Ã—â€¯training MAE**.
- New model registered and autoâ€‘promoted if it beats previous validation MAE.

---

## Inference Pipeline

- Runs **hourly** via GitHub Action (`5 * * * *`).
- Retrieves latest hourâ€™s features, predicts next hour, writes to `bike_demand_predictions` (offlineÂ +Â online).

---

## Dashboards (Streamlit CommunityÂ Cloud)

| App                     | Purpose                                                                             |
| ----------------------- | ----------------------------------------------------------------------------------- |
| **frontend_monitor.py** | Displays MAEâ€¯/â€¯MAPE over a selectable window; pulls data from feature store.        |
| **frontend_v2.py**      | Interactive circleâ€‘map of station demand, topâ€‘N lists, and timeâ€‘series drillâ€‘downs. |

Secrets required in Streamlit: `HOPSWORKS_API_KEY`, `HOPSWORKS_PROJECT_NAME`.

---

## GitHubÂ Actions Schedules (UTC)

- **Feature engineering**Â â€”Â `30Â 0Â *Â *Â *`
- **Model training**Â â€”Â `0Â 1Â *Â *Â *`
- **Inference**Â â€”Â `5Â *Â *Â *Â *`

---

## Dependencies

- PythonÂ 3.12
- Key libsÂ : `pandas`, `pyarrow`, `lightgbm`, `optuna`, `plotly`, `folium`, `geopandas`, `streamlit`, `hopsworks`.

---

## Credentials & Secrets

| Secret                     | Where used                |
| -------------------------- | ------------------------- |
| **HOPSWORKS_API_KEY**      | GitHubÂ Actions, Streamlit |
| **HOPSWORKS_PROJECT_NAME** | GitHubÂ Actions, Streamlit |

---

## Firstâ€‘Run Checklist

1. Place raw CSVs in `data/raw/2023â€‘*`.
2. Run consolidation script â†’ verify Parquet & duplicate log.
3. Execute notebooksâ€¯02â€¯â†’â€¯05 once to generate tabular Parquet.
4. Register `bike_hourly_fg` via notebookâ€¯12.
5. Run hyperâ€‘parameter tuning (notebookâ€¯20) â†’ model registry.
6. Add Hopsworks secrets to GitHub & Streamlit.
7. Enable GitHubÂ Actions workflows.
8. Deploy Streamlit app; confirm live predictions & MAE metrics.
9. Monitor drift; enjoy ðŸš´ðŸ“ˆ!
